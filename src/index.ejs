<!doctype html>

<!-- Any figures to-be called with <img ...> should be placed in /static and called
as with /static as their root. E.g. <img src="/diagrams/fig1.png">
-->

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="template.v2.js"></script>
  <title>arXiv/viXra - Baseline Models - Garrett Goon</title>
</head>

<body>

  <!-- I added a date field to more easily add the date to the front matter. Removed the DOI fiel -->

  <d-front-matter>
    <script type="text/json">
      {
        "title": "arXiv/viXra - Baseline Models",
        "description": "An example project using Webpack, Babel, and Svelte.",
        "authors": [
          {
            "author": "Garrett Goon",
            "authorURL": "https://garrettgoon.com",
            "affiliation": "CMU",
            "affiliationURL": "https://www.cmu.edu/physics/"
          }
        ],
        "katex": {
          "delimiters": [
            {
              "left": "$",
              "right": "$",
              "display": false
            },
            {
              "left": "$$",
              "right": "$$",
              "display": true
            }
          ]
        },
        "date" : "15 December, 2021"
      }
  </script>
  </d-front-matter>

  <d-title>
    <h1>arXiv/viXra - Baseline Models</h1>
    <p>Logistic Regression and Random Forest</p>
  </d-title>

  <d-article>

    <p>
      <em>
        <u>Note:</u>
      </em> The Jupyter/Colab notebooks relevant to this post are
      <a target='_blank' rel='noopener noreferrer'
        href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/simple_baselines'> here on my GitHub page.</a>
      My <d-code language='python'>pytorch_lightning</d-code> implementation of a basic feed-forward network for one-hot
      encoded text (of which logistic regression is a limit)
      <a target='_blank' rel='noopener noreferrer'
        href='https://github.com/garrett361/arxiv-vixra-ml/blob/main/arxiv_vixra_models/baseline_models.py'>can be
        found here.</a>
    </p>

    <h3>
      Starting Simple
    </h3>



    <p>
      There are advantages to starting a project by creating some simple models, rather than immediately diving in to
      more complicated ones:
    </p>
    <ul>
      <li>
        Simple models provide a baseline for later comparison and can be surprisingly effective (as in the present
        case).
      </li>
      <li>
        Simple models are often much easier to interpret. This can be useful for revealing issues in the raw data itself
        or in its processing, in addition to the intrinsic appeal of interpretability.
      </li>
    </ul>

    <p>
      Below I detail the results of applying logistic regression and a Random Forest to character-level, one-hot encoded
      arXiv/viXra title data.
    </p>

    <h3>
      Logistic Regression
    </h3>

    <p>
      The simplest version of a fully-connected model for text analysis is the following. After one-hot encoding the
      titles, all of which are padded or truncated to a fixed length, <d-code language='python'>seq_len</d-code>, each
      data
      point is a <d-code language='python'>(seq_len, chars)</d-code>-shaped tensor, with
      <d-code language='python'>chars</d-code> the number of unique characters used in the encoding (see <a
        target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-data/'>the Data post</a>).
      The resulting tensor can then be flattened into a vector<d-footnote id='tensor-conventions'>
        There are various differences in the tensor/array conventions and terminology used in physics and ML. In
        particular, what physicists would call a <d-math>D</d-math>-dimensional vector <d-math>\vec{v}</d-math> whose
        components are <d-math>v_{i}</d-math> with a single index <d-math>i\in\{0,\ldots, D-1\}</d-math>, is often
        represented to in ML circles as a <d-code language='python'>(1, D)</d-code>- or
        <d-code language='python'>(D, 1)</d-code>-shaped tensor (in <d-code language='python'>numpy/pytorch</d-code>
        language) with a singleton dimension explicitly noted, meaning that they are really tensors of
        the form <d-math>v_{ij}</d-math> or <d-math>v_{ji}</d-math> with <d-math>i</d-math> as above and <d-math>
          j\in\{0\}</d-math>, strictly speaking (vectors which truly only have a single index can also be created, and
        are denoted as being
        <d-code language='python'>(D, )</d-code>-shaped, but these other options seems to appear more frequently).
        Tracking such information is important in ML for
        broadcasting purposes and it takes a little time to get used to, if coming from the more concise physics
        conventions.
      </d-footnote>
      <d-math>\vec{t}\in \mathbb{R}^{\texttt{seq\_len}\times \texttt{chars}}</d-math> and
      probability that a given paper title <d-math>\vec{t}</d-math> is from viXra is finally modeled by
      <d-footnote id='other-conventions'>
        Other conventions: <d-math>\cdot</d-math> is used both for matrix multiplication and dot products as in <d-math>
          W\cdot \vec{x}</d-math> or <d-math>\vec{x}\cdot \vec{y}</d-math>, while <d-math>*</d-math> represents
        element-wise multiplication, also known as the Hadamard product, <d-math>(A* B)_{ij}\equiv A_{ij}\times
          B_{ij}</d-math>. ML would benefit from greater use of the Einstein summation convention (<a target='_blank'
          rel='noopener noreferrer' href='https://pytorch.org/docs/stable/generated/torch.einsum.html'>implemented in
          <d-code language='python'>pytorch</d-code> by <d-code language='python'>einsum</d-code>!
        </a>) in which the usual summation symbol is left implicit in any tensor sums over indices, as the presence of
        repeated indices already implies the existence of a sum: <d-math>\sum_{i,j}T_{abij}U_{cidje}\longrightarrow
          T_{abij}U_{cidje}</d-math>. This is often (semi-)jokingly referred to as Einstein's greatest contribution to
        physics. The prevalence of Hadamard products in ML does make its use a bit more confusing in that
        context, however.
      </d-footnote>
    </p>

    <d-math block=''>
      P(\vec{t}) = \sigma\left(\vec{W}\cdot\vec{t}+b\right) \ , \quad \sigma(x)\equiv \frac{1}{1+e^{-x}}\ .
    </d-math>
    <p>
      Above <d-math>\vec{W}\in\mathbb{R}^{\texttt{seq\_len}\times \texttt{chars}}</d-math> is simply another vector (the
      weights) and <d-math>b</d-math>
      is a scalar (the bias), both of which are to be tuned by minimizing the cross-entropy loss<d-footnote
        id='cross-entropy-loss'>
        Given <d-math>D</d-math> data points belonging to <d-math>C</d-math> classes, respectively indexed by <d-math>
          \alpha \in
          \{0,\ldots, D-1\}</d-math> and <d-math>i \in \{0,\ldots, C-1\}</d-math>, and a model which assigns
        corresponding class probabilities to each example <d-math>q_\alpha^i</d-math>, the to-be-minimized empirical
        cross-entropy loss is
        <d-math block=''>
          H_{\rm empirical}(q)\propto -\sum_\alpha \ln q_\alpha^{i(\alpha)}
        </d-math>
        where <d-math>i(\alpha)</d-math> is the true class-label for the <d-math>\alpha</d-math>-th data point. This
        metric is only sensitive to how poor a model's prediction for the true class label was, the distribution of
        probabilities for incorrect class labels being totally irrelevant.
        <br>
        <br>
        <d-math>H_{\rm empirical}(q)</d-math> is an approximation to the cross-entropy loss <d-math>H(p,q)</d-math>,
        <d-math block=''>
          H(p,q)\equiv -\sum_i p_i \ln q_i\ ,
        </d-math>
        for two distributions specified by <d-math>p_i</d-math> and <d-math>q_i</d-math>. In the present context
        the observed data points are assumed generated according to the <d-math>p_i</d-math>. Minimizing <d-math>H_{\rm
          empirical}(q)</d-math> by tuning the parameters which determine the <d-math>q_\alpha^i</d-math> is equvalent
        to minimizing the negative <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood'>log-likelihood</a> or empirically
        estimated <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence'>KL-divergence</a> (more commonly called the
        Relative Entropy in physics): <d-math>D_{\rm
          KL}(P||Q)=-\sum_i p_i \ln \left(q_i/p_i\right)</d-math>.
        <br>
        <br>
        <d-math>D_{\rm
          KL}(P||Q)</d-math> is often colloquially referred to as a distance-measure between two distributions, since it
        vanishes if <d-math>p_i=q_i\ , \forall \ i</d-math> and is positive otherwise, though it does not satisfy the
        other usual properties of distances. Instead, the KL-divergence is better thought of as the rate at
        which one will realize their error if they mistakenly model some process using probabilities <d-math>q_i
        </d-math>
        when in reality it is being generated according to probabilities <d-math>p_i</d-math>; see <a target='_blank'
          rel='noopener noreferrer'
          href='https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf'>Jared Kaplan's
          notes for a nice discussion on this point.</a>
      </d-footnote>, as usual.
    </p>

    <p>
      This model, known as logistic regression, can only pick up on technical data in the text and not its semantics,
      since it has no means for efficiently
      learning about interactions between characters at different positions in the title. Nevertheless, it performs
      surprisingly well (to me, at
      least), achieving <d-math>\approx 70\%</d-math> accuracy on the validation set! For reference, humans guessing at
      <a target='_blank' rel='noopener noreferrer'
        href='https://garrettgoon.com/arxiv-vixra-quiz/'>garrettgoon.com/arxiv-vixra/quiz</a> also only guess titles
      sources correctly <d-math>
        \approx 70\%</d-math> of the time. The evolution of the
      confusion matrix throughout training is
      below <a target='_blank' rel='noopener noreferrer'
        href='https://wandb.ai/garrett361/arxiv_vixra_examples/reports/Logistic-Regression-Confusion-Matrix--VmlldzoxNDU2NjA0'>(direct
        link here)</a>.
    </p>

    <iframe
      src="https://wandb.ai/garrett361/arxiv_vixra_examples/reports/Logistic-Regression-Confusion-Matrix--VmlldzoxNDU2NjA0"
      style="border:none;height:1024px;width:100%">
    </iframe>

    <p>
      Because of the simplicity of the model, it is very easy to interpret. Every entry of <d-math>\vec{W}</d-math>
      corresponds to a single character at a single position in the text. A positive weight indicates that having that
      particular character at that specific position will push the prediction toward viXra, with the analogous result
      for negative values and arXiv.
    </p>

    <p>
      In the images below are the five most-positive and most-negative characters by weight for the last character in
      all titles
      (left image), for characters averaged over all possible locations in a title (middle image), and,
      finally, characters in the first possible location in a title (right image). Since all titles were forced to be
      of
      the length <d-code language='python'>seq_len = 128</d-code> with spaces inserted on the left as padding (when
      necessary), the last character in each sequence should be much more
      important <d-footnote id='padding'>
        Taking the mean over characters, I have also verified that the largest weights by magnitude
        correspond to positions near the end of titles, meaning those are the most informative locations, as one would
        expect.
      </d-footnote> than the first character, and this is
      clearly borne out.
    </p>


    <figure style="grid-column-end: page-end">
      <img src='images/last_char_weights.png' alt='' style='width: 30%'>
      <img src='images/mean_position_weights.png' alt='' style='width: 29.3%'>
      <img src='images/first_char_weights.png' alt='' style='width: 29.6%'>
      <figcaption>
        Values of the trained weight vector <d-math>\vec{W}</d-math> for various characters corresponding to the final
        character (left), the mean over all character positions (middle), and for the first character (right). The
        three
        signals are ordered by decreasing importance, as seen from their typical magnitudes.
      </figcaption>
    </figure>

    <p>
      The model has picked up on the fact that viXra papers are more likely to end with a period, exclamation point,
      or
      other punctuation marks, while arXiv papers are more likely to end with some numerical value or a dollar-sign
      (presumably due to <d-code language='python'>LaTeX</d-code> equation setting). Averaging over all positions,
      viXra
      papers are more likely to have commas anywhere while most of the top arXiv markers are again associated with
      <d-code language='python'>LaTeX</d-code>. These <d-code language='python'>LaTeX</d-code> signals will be a
      persistent theme. The below images inspect these findings in further detail.
    </p>


    <figure style="grid-column-end: page-end">
      <img src='images/mean_position_contains_dollar.png' alt='dollar latex examples'>
      <figcaption>
        Examples of titles which contain <d-math>\$</d-math> and the relevant conditional probabilities. Empirically,
        a
        <d-math>\$</d-math> in a title indicates a <d-math>\sim 96\%</d-math> chance that the paper is arXiv, but only
        <d-math>\sim 2\%</d-math> of all titles have a dollar-sign anywhere.
      </figcaption>
    </figure>

    <figure style="grid-column-end: page-end">
      <img src='images/fc_ends_with_bang.png' alt='dollar latex examples'>
      <figcaption>
        Examples of titles which end with <d-math>!</d-math> and the relevant conditional probabilities. Empirically,
        a
        <d-math>!</d-math> ending a title indicates a <d-math>89\%</d-math> chance that the paper is viXra, but only
        a tiny fraction of all papers end with an exclamation point.
      </figcaption>
    </figure>

    <p>
      Finally, logistic regression has picked up on the fact that viXra papers tend to be shorter than arXiv ones, as
      indicated by the fact that they typically have more left-padding by blank spaces when forced to be of length
      <d-code language='python'>seq_len = 128</d-code>. In the figure below, the top plot shows the weights assigned to
      blank space summed from position zero in the sequence up to the position marked on the horizontal axis. The sum
      initially trends towards arXiv and flattens out position <d-math>\sim 60</d-math>
      and then strongly reverses course towards viXra around positions <d-math>\sim 75</d-math> and beyond.
      Correspondingly, titles whose non-trivial content starts somewhere before this latter dividing line have
      accumulated a bias towards arXiv, while those which start somewhat after this point are weighted towards viXra.
      Not
      coincidentally,
      the scales of these inflection points coincide with the mean start positions for arXiv and viXra titles, se the
      vertical lines in the plot show.
      The bottom plot demonstrates similar behavior for the average non-blank-space weights summed from the end of the
      sequence
      to a given position.
      In the next section, we will again see that character length is one of the primary predictors for a title's
      origin.
    </p>

    <figure style='grid-column-end: page-end' id='logistic-charlen'>
      <img src='images/balanced_title_linear_one_hot_cumulative_weights_by_position.svg'
        alt='Weights for blank spaces, logistic regression.'>
      <figcaption>
        Top plot: the cumulative sum of weights corresponding to all blank spaces, starting from position <d-code
          language='python'>0</d-code>, described in more detail above. Bottom plot: the cumulative sum of the mean of
        all non-blank-space weights, starting from position <d-code language='python'>128</d-code>. Since all titles end
        with non-blank characters, the sum is non-informative towards the end of the sequence. As the title length
        increases and one moves to the left of the plot, the sum increasingly indicates an arXiv paper.
      </figcaption>
    </figure>

    <h3>
      Random Forest
    </h3>

    <p>
      I used a Random Forest<d-footnote id='rf'>
        In brief, a Random Forest classifier works by generating a large number of decision trees which are each trained
        using a
        randomly selected set of training examples and a randomly selected subset of data features. The conditions for
        generating a terminating leaf
        are controlled by various hyperparameters, such as the maximum depth of the tree or number of training examples
        remaining post-split. In a binary classification context, the
        Random Forest then makes a prediction for a new data
        point by passing it through all of the collected trees and taking a majority vote based on the various leaves
        the data point settles in.
        The random selection of data points and features for each tree and the averaging procedure at inference time
        are the key features which decorrelate trees and combat variance.
      </d-footnote> as a second baseline model. In order to facilitate training, I also
      performed a modest amount of feature engineering such as computing the frequency with which
      various forms of punctuation appear in titles and the length of the title's longest word, resulting in 64 total
      features. See the figure below for the data distributions along some particular engineered feature-directions.
      The ultimate model (which only uses ten of the 64 features, discussed below) achieved approximately the same
      validation-set
      performance as logistic regression: <d-math>\approx 70\%
      </d-math> accuracy.

    </p>

    <figure style='grid-column-end: page-end' id='rf-corner'>
      <img src='images/rf_feature_engineering_corner_plot.png' alt='feature engineering for the random forest'>
      <figcaption>
        A corner plot of a few features used by the Random Forest. <d-code language='python'>mean_word_rank</d-code>
        is
        the average rank of all words in each title, as computed from the ranks of how common each word is in the
        training set.
      </figcaption>
    </figure>


    <p>
      Sixty-four features is too many to be useful for a simple quick-and-dirty baseline. Apart from some quantitative
      benchmarks, one also wants a greater understanding of what features are <em>important</em> in distinguishing arXiv
      papers from viXra, and most of the 64 are not.
    </p>

    <p>
      There exist two common methods for quantifying the importance of different features in a Random Forest:
    </p>

    <ul>
      <li>
        <b>Gini-Impurity-Based Importance:</b> in <d-code language='python'>sklearn</d-code>, the split-points in trees
        are determined by maximizing Gini Impurity<d-footnote id='gini'>
          The Gini Impurity of a set of data points belonging to <d-math>C</d-math> classes, indexed by <d-math>i
            \in\{0,\ldots, C-1\}</d-math>, is
          given by the probability of misclassifying a randomly chosen data point when using a simple strategy in
          which
          you
          randomly guess which class the point belongs to in proportion to the relative class populations of the whole
          set. That is, if <d-math>p_i</d-math> is the probability of drawing from class <d-math>i</d-math>, then
          the
          impurity is given by
          <d-math block=''>
            I_G = \sum_i P({\rm misclassify}|c_i)P(c_i) =\sum_i (1-p_i)p_i\ .
          </d-math>
          If the
          data set only has elements from one class, then <d-math>I_G</d-math> is minimized at zero, while an
          equally-balanced dataset has maximal <d-math>I_G</d-math>. A Random Forest strives to make splits such that
          all examples which end up in a leaf of a tree belong to the same class, which is equivalent to minimizing
          <d-math>I_G</d-math>.
        </d-footnote> decrease by default. By creating a weighted average of the Gini Impurity decrease achieved by
        every split
        in every tree per split-upon-feature, one estimates the relative importance of all features.
        <d-code language='python'>sklearn</d-code> implements this method via <a target='_blank'
          rel='noopener noreferrer'
          href='https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_'>
          <d-code language='python'>feature_importances_</d-code>
        </a>.
      </li>
      <li>
        <b>Permutation-Based Importance</b>: a more computationally-expensive metric for assessing feature importance is
        to calculate the drop in performance which comes from randomly shuffling the values for a single feature in some
        validation set, repeating this process for each feature.
        <d-code language='python'>sklearn</d-code> implements this method via <a target='_blank'
          rel='noopener noreferrer'
          href='https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance'>
          <d-code language='python'>permutation_importance</d-code>
        </a>
      </li>
    </ul>

    <p>
      The Gini-Impurity-based method <a target='_blank' rel='noopener noreferrer'
        href='https://explained.ai/rf-importance/index.html'>has been criticized</a> as overestimating the importance of
      features which allow for many possible choices of split-points. As done in the preceding link, this effect can be
      seen by adding a test-feature which consists solely of randomly generated numbers drawn from a simple Gaussian.
      Adding such a feature to the
      data and computing feature importances as above, the Gini method ranked this
      <d-code language='python'>random</d-code> feature as the 17th most-important feature, while permutation importance
      more-reasonably has it
      ranked dead-last: 65 of 65. See the plot of permutation-based importances below.
    </p>

    <figure style='grid-column-end: page-end' id='perm-import-all'>
      <img src='images/rf_0_perm_import_plt.svg' alt='plot of all permutation based importances'>
      <figcaption>
        The losses in accuracy when different feature dimensions are shuffled. Multiple shuffle-iterations are performed
        for each feature and the error bars come from the standard deviation across shuffles. Shuffling the
        <d-code language='python'>random</d-code> feature improves performance, presumably since it washes out the
        unwanted effects created by training on this uninformative column in the first place.
      </figcaption>
    </figure>

    <p>
      There are clearly many uninformative features. As a quick-and-dirty contraction of the model, I re-trained the
      Random Forest on the top-ten<d-footnote id='removing-ten'>
        Reducing the number of features slightly <em>increased</em> the performance of the model. In general, one should
        also carefully consider the correlations between different features before winnowing down the elements which are
        fed into a Random Forest, but taking the top-ten was good-enough for present purposes.
      </d-footnote> features per the above chart, resulting in the more-focused importances plot below.
      As found in the logistic regression model, many of the most-informative markers regarded the use of punctuation
      marks and the overall length of the title. Unlike logistic regression, the Random Forest also had access to how
      common the words in each title were ( <d-code language='python'>mean_word_rank</d-code>), which also carried
      significant meaning, as one might have suspected.
    </p>

    <figure style='grid-column-end: page-end' id='perm-import-all'>
      <img src='images/rf_1p_perm_import_plt.svg' alt='plot of all permutation based importances'>
      <figcaption>
        Permutation-based importances for the ten most-important columns.
      </figcaption>
    </figure>

    <p>
      Lastly, it can be informative to get an inside-view of what is going on in the various trees which comprise a
      Random Forest.
      The <a target='_blank' rel='noopener noreferrer' href='https://github.com/parrt/dtreeviz'>
        <d-code language='python'>dtreeviz</d-code> package
      </a> is a useful tool for this purpose. There exist methods for visualizing how groups of data or individual
      titles filter through the leaves of the tree. An example of the path that one of my own papers takes when
      traversing a single tree is below. A visualization of 500 validation-set samples being filtered into the leaves
      of the same tree is a bit more unwieldy, <a target='_blank' rel='noopener noreferrer'
        href='images/rf_1p_rand_tree_full_viz.svg'>but can be found here.</a>
    </p>

    <figure style='grid-column-end: page-end' id='dtreeviz-goon'>
      <img src='images/rf_1p_rand_tree_viz.svg' alt='dtreeviz visualization of a single paper prediction'>
      <figcaption>
        An (incorrect) prediction-path for one of my papers (<a target='_blank' rel='noopener noreferrer'
          href='https://arxiv.org/abs/1201.0015'><em>
            Gauged Galileons From Branes</em></a>) from one tree in the Random Forest, as visualized by <d-code
          language='python'>dtreeviz</d-code>. The yellow bars indicate the distribution of
        all twenty of my papers, while the orange arrow points to the statistics of <em>
          Gauged Galileons From Branes</em> and the black arrow marks the split-points.
      </figcaption>
    </figure>




    <h3>
      Performance on My Papers
    </h3>

    <p>
      I, of course, have to check the model performance <a target='_blank' rel='noopener noreferrer'
        href='https://inspirehep.net/literature?sort=mostrecent&size=25&page=1&q=a%20g%20goon&ui-citation-summary=true'>on
        my own papers</a> (which are all arXiv), which I will do for each model in this series of posts.
    </p>

    <p>
      The results aren't great for these baselines! Both models only classify half of my papers correctly. As seen in
      the figures below, there
      is a clear trend for longer titles to be predicted as arXiv and shorter ones as viXra, as expected from the
      above. This might be expected, given the lack of
      <d-code language='python'>LaTeX</d-code> and related technical markers in the titles of my
      papers.
    </p>

    <figure style='grid-column-end: page-end' id='logistic-preds'>
      <img src='images/balanced_title_linear_one_hot_goon_papers_preds.svg' alt='logistic predictions'>
      <figcaption>
        Probabilities that each of my titles are viXra, as predicted by the logistic regression model.
      </figcaption>
    </figure>

    <figure style='grid-column-end: page-end' id='rf-preds'>
      <img src='images/goon_papers_df_tree_pred_plot.svg' alt='random forest predictions'>
      <figcaption>
        Probabilities that each of my titles are viXra, as predicted by the Random Forest, as estimated by computing
        the
        mean prediction over all trees in the forest.
      </figcaption>
    </figure>


  </d-article>



  <d-appendix>


    <h3>Acknowledgments</h3>

    <p>
      Thank you to <a href="https://distill.pub" target="_blank" rel="noopener noreferrer">the <em>Distill</em> team</a>
      for making their
      <a href="https://github.com/distillpub" target="_blank" rel="noopener noreferrer">article template publicly
        available.</a> Discussions with Matt Malloy and Rami Vanguri for gaining an intuition for the various
      hyperparameters used in a Random Forest.
    </p>


    <h3>Additional Links</h3>


    <ul class="color-dot-ul">
      <li>
        Jeremy Howard has great videos on the practicalities of (and some theory behind) Random Forests, such as <a
          target='_blank' rel='noopener noreferrer'
          href='https://www.youtube.com/watch?v=blyXCk4sgEg&list=PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96&index=2'>here</a>
        and <a target='_blank' rel='noopener noreferrer'
          href='https://www.youtube.com/watch?v=VEG5xT5gAHc&list=PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM&index=7'>here</a>.
        Notes on Random Forest hyperparameter tuning from his <d-code language='python'>fastai</d-code> course <a
          target='_blank' rel='noopener noreferrer' href='https://forums.fast.ai/t/wiki-lesson-thread-lesson-4/7540'>can
          be found here</a>.
      </li>
      <li>
        Random forests are also useful for analyzing the performance other ML architectures. Weights and Biases (used
        extensively for
        this project, see the <a target='_blank' rel='noopener noreferrer'
          href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow post</a>) uses Random Forests to estimate the
        importance of various
        hyperparameters across model training runs <a target='_blank' rel='noopener noreferrer'
          href='https://docs.wandb.ai/ref/app/features/panels/parameter-importance#interpreting-a-hyperparameter-importance-panel'>via
          the permutation-based method detailed above</a>.
      </li>
    </ul>

    <d-footnote-list></d-footnote-list>

    <h3>All Project Posts</h3>

    <p>Links to all posts in this series.
      <em>Note: all code for this project can be found <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml'>on my GitHub page</a>.
      </em>
    </p>

    <ul>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-data/">The Data</a>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-baseline-models/">Baseline Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-simple-recurrent/">Simple
          Recurrent Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-embeddings/">Embeddings</a>
      </li>
      <li>
        ...in progress...
      </li>
      <li>
        Test Set Performance and Conclusions
      </li>
    </ul>
    <d-citation-list></d-citation-list>
  </d-appendix>


  <!-- bibliography will be inlined during Distill pipeline's pre-rendering
    (GG: I have not managed to get the bibliography to compile after the ejs
     is converted to a static html file, so commenting out)
  <d-bibliography src="bibliography.bib"></d-bibliography>

   -->


</body>
