<!doctype html>

<!-- Any figures to-be called with <img ...> should be placed in /static and called 
as with /static as their root. E.g. <img src="/diagrams/fig1.png">
-->

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="template.v2.js"></script>
  <title>arXiv/viXra - Baseline Models - Garrett Goon</title>
</head>

<body>

  <!-- I added a date field to more easily add the date to the front matter. Removed the DOI fiel -->

  <d-front-matter>
    <script type="text/json">
      {
        "title": "arXiv/viXra - Baseline Models",
        "description": "An example project using Webpack, Babel, and Svelte.",
        "authors": [
          {
            "author": "Garrett Goon",
            "authorURL": "https://garrettgoon.com",
            "affiliation": "CMU",
            "affiliationURL": "https://www.cmu.edu/physics/"
          }
        ],
        "katex": {
          "delimiters": [
            {
              "left": "$",
              "right": "$",
              "display": false
            },
            {
              "left": "$$",
              "right": "$$",
              "display": true
            }
          ]
        },
        "date" : "15 December, 2021"
      }
  </script>
  </d-front-matter>

  <d-title>
    <h1>arXiv/viXra - Baseline Models</h1>
    <p>Logistic Regression and Random Forest</p>
  </d-title>

  <d-article>

    <p>
      <em>
        <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/simple_baselines'>Note: The Jupyter/Colab
          notebooks
          relevant to this post can be found on my GitHub page.</a>
      </em>
    </p>

    <h3>
      Starting Simple
    </h3>



    <p>
      There are advantages to starting a project by creating some simple models, rather than immediately diving in to
      more complicated ones:
    </p>
    <ul>
      <li>
        Simple models provide a baseline for later comparison and can be surprisingly effective (as in the present
        case).
      </li>
      <li>
        Simple models are often much easier to interpret. This can be useful for revealing issues in the raw data itself
        or in its processing, in addition to the intrinsic appeal of interpretability.
      </li>
    </ul>

    <p>
      Below I detail the results of applying logistic regression and a Random Forest to character-level, one-hot encoded
      arXiv/viXra title data.
    </p>

    <h3>
      Logistic Regression
    </h3>

    <p>
      The simplest version of a fully-connected model for text analysis is the following. After one-hot encoding the
      titles, all of which are padded or truncated to a fixed length, <d-code language='python'>seq_len</d-code>, each
      data
      point is a <d-code language='python'>(seq_len, chars)</d-code>-shaped tensor, with
      <d-code language='python'>chars</d-code> the number of unique characters used in the encoding (see <a
        target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-data/'>the Data post</a>).
      The resulting tensor can then be flattened into a vector<d-footnote id='tensor-conventions'>
        There are various differences in the tensor/array conventions and terminology used in physics and ML. In
        particular, what physicists would call a <d-math>D</d-math>-dimensional vector <d-math>\vec{v}</d-math> whose
        components are <d-math>v_{i}</d-math> with a single index <d-math>i\in\{0,\ldots, D-1\}</d-math>, is often
        represented to in ML circles as a <d-code language='python'>(1, D)</d-code>- or
        <d-code language='python'>(D, 1)</d-code>-shaped tensor (in <d-code language='python'>numpy/pytorch</d-code>
        language) with a singleton dimension explicitly noted, meaning that they are really tensors of
        the form <d-math>v_{ij}</d-math> or <d-math>v_{ji}</d-math> with <d-math>i</d-math> as above and <d-math>
          j\in\{0\}</d-math>, strictly speaking.
        Tracking such information is important in ML for
        broadcasting purposes and it takes a little time to get used to, if coming from the more concise physics
        conventions.
      </d-footnote>
      <d-math>\vec{t}\in \mathbb{R}^{\texttt{seq\_len}\times \texttt{chars}}</d-math> and
      probability that a given paper title <d-math>\vec{t}</d-math> is from viXra is finally modeled by
      <d-footnote id='other-conventions'>
        Other conventions: <d-math>\cdot</d-math> is used both for matrix multiplication and dot products as in <d-math>
          W\cdot \vec{x}</d-math> or <d-math>\vec{x}\cdot \vec{y}</d-math>, while <d-math>\ast</d-math> represents
        element-wise multiplication, also known as the Hadamard product, <d-math>(A\ast B)_{ij}\equiv A_{ij}\times
          B_{ij}</d-math>. ML would benefit from greater use of the Einstein summation convention (<a target='_blank'
          rel='noopener noreferrer' href='https://pytorch.org/docs/stable/generated/torch.einsum.html'>implemented in
          <d-code language='python'>pytorch</d-code> by <d-code language='python'>einsum</d-code>!
        </a>) in which the usual summation symbol is left implicit in any tensor sums over indices, as the presence of
        repeated indices already implies the existence of a sum: <d-math>\sum_{i,j}T_{abij}U_{cidje}\longrightarrow
          T_{abij}U_{cidje}</d-math>. This is often (semi-)jokingly referred to as Einstein's greatest contribution to
        physics, though the prevalence of Hadamard products in ML does make its use a bit more confusing in that
        context.
      </d-footnote>
    </p>

    <d-math block=''>
      P(\vec{t}) = \sigma\left(\vec{W}\cdot\vec{t}+b\right) \ , \quad \sigma(x)\equiv \frac{1}{1+e^{-x}}\ .
    </d-math>
    <p>
      Above <d-math>\vec{W}\in\mathbb{R}^{\texttt{seq\_len}\times \texttt{chars}}</d-math> is simply another vector (the
      weights) and <d-math>b</d-math>
      is a scalar (the bias), both of which are to be tuned by minimizing the cross-entropy loss<d-footnote
        id='cross-entropy-loss'>
        Given <d-math>D</d-math> data points belonging to <d-math>C</d-math> classes, respectively indexed by <d-math>
          \alpha \in
          \{0,\ldots, D-1\}</d-math> and <d-math>i \in \{0,\ldots, C-1\}</d-math>, and a model which assigns
        corresponding class probabilities to each example <d-math>q_\alpha^i</d-math>, the to-be-minimized empirical
        cross-entropy loss is
        <d-math block=''>
          H_{\rm empirical}(q)\propto -\sum_\alpha \ln q_\alpha^{i(\alpha)}
        </d-math>
        where <d-math>i(\alpha)</d-math> is the true class-label for the <d-math>\alpha</d-math>-th data point. This
        metric is only sensitive to how poor a model's prediction for the true class label was, the distribution of
        probabilities for incorrect class labels being totally irrelevant.
        <br>
        <br>
        <d-math>H_{\rm empirical}(q)</d-math> is an approximation to the cross-entropy loss <d-math>H(p,q)</d-math>,
        <d-math block=''>
          H(p,q)\equiv -\sum_i p_i \ln q_i\ ,
        </d-math>
        for two distributions specified by <d-math>p_i</d-math> and <d-math>q_i</d-math>. In the present context
        the observed data points are assumed generated according to the <d-math>p_i</d-math>. Minimizing <d-math>H_{\rm
          empirical}(q)</d-math> by tuning the parameters which determine the <d-math>q_\alpha^i</d-math> is equvalent
        to minimizing the negative <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood'>log-likelihood</a> or empirically
        estimated <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence'>KL-divergence</a> (more commonly called the
        Relative Entropy in physics): <d-math>D_{\rm
          KL}(P||Q)=-\sum_i p_i \ln \left(q_i/p_i\right)</d-math>.
        <br>
        <br>
        <d-math>D_{\rm
          KL}(P||Q)</d-math> is often colloquially referred to as a distance-measure between two distributions, since it
        vanishes if <d-math>p_i=q_i\ , \forall \ i</d-math> and is positive otherwise, though it does not satisfy the
        other usual properties of distances. Instead, the KL-divergence is better thought of as the rate at
        which one will realize their error if they mistakenly model some process using probabilities <d-math>q_i
        </d-math>
        when in reality it is being generated according to probabilities <d-math>p_i</d-math>; see <a target='_blank'
          rel='noopener noreferrer'
          href='https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf'>Jared Kaplan's
          notes for a nice discussion on this point.</a>
      </d-footnote>, as usual.
    </p>

    <p>
      This model, known as logistic regression, can only pick up on technical data in the text, and not its semantics,
      since it has no means for efficiently
      learning about interactions between characters at different positions in the title. Nevertheless, it performs
      surprisingly well (to me, at
      least), achieving <d-math>\approx 70\%</d-math> accuracy on the validation set! The evolution of the
      confusion matrix throughout training is
      below <a target='_blank' rel='noopener noreferrer'
        href='https://wandb.ai/garrett361/balanced_title_linear_one_hot/reports/Logistic-Regression-Confusion-Matrix--VmlldzoxMzg1NzE3'>(direct
        link here)</a>.
    </p>

    <iframe src="https://wandb.ai/garrett361/balanced_title_linear_one_hot/reports/Logistic-Regression-Confusion-Matrix--VmlldzoxMzg1NzE3" style="border:none;height:1024px;width:100%">
      </iframe>

    <p>
      Because of the simplicity of the model, it is very easy to interpret. Every entry of <d-math>\vec{W}</d-math>
      corresponds to a single character at a single position in the text. A positive weight indicates that having that
      particular character at that specific position will push the prediction toward viXra, with the analogous result
      for negative values and arXiv.
    </p>

    <p>
      In the images below are the five most-positive and most-negative characters by weight for the last character in
      all titles
      (left image), for characters averaged over all possible locations in a title (middle image), and,
      finally, characters in the first possible location in a title (right image). Since all titles were forced to be
      of
      the length <d-code language='python'>seq_len = 128</d-code> with spaces inserted on the left as padding (when
      necessary), the last character in each sequence should be much more
      important <d-footnote id='padding'>
        Taking the mean over characters, I have also verified that the largest weights by magnitude
        correspond to positions near the end of titles, meaning those are the most informative locations, as one would
        expect.
      </d-footnote> than the first character, and this is
      clearly borne out.
    </p>


    <figure style="grid-column-end: page-end">
      <img src='images/last_char_weights.png' alt='' style='width: 30%'>
      <img src='images/mean_position_weights.png' alt='' style='width: 29.3%'>
      <img src='images/first_char_weights.png' alt='' style='width: 29.6%'>
      <figcaption>
        Values of the trained weight vector <d-math>\vec{W}</d-math> for various characters corresponding to the final
        character (left), the mean over all character positions (middle), and for the first character (right). The
        three
        signals are ordered by decreasing importance, as seen from their typical magnitudes.
      </figcaption>
    </figure>

    <p>
      The model has picked up on the fact that viXra papers are more likely to end with a period, exclamation point,
      or
      other punctuation marks, while arXiv papers are more likely to end with some numerical value or a dollar-sign
      (presumably due to <d-code language='python'>LaTeX</d-code> equation setting). Averaging over all positions,
      viXra
      papers are more likely to have commas anywhere while most of the top arXiv markers are again associated with
      <d-code language='python'>LaTeX</d-code>. These <d-code language='python'>LaTeX</d-code> signals will be a
      persistent theme. The below images inspect these findings in further detail.
    </p>


    <figure style="grid-column-end: page-end">
      <img src='images/mean_position_contains_dollar.png' alt='dollar latex examples'>
      <figcaption>
        Examples of titles which contain <d-math>\$</d-math> and the relevant conditional probabilities. Empirically,
        a
        <d-math>\$</d-math> in a title indicates a <d-math>96\%</d-math> chance that the paper is arXiv, but only
        <d-math>5\%</d-math> of all titles have a dollar-sign anywhere.
      </figcaption>
    </figure>

    <figure style="grid-column-end: page-end">
      <img src='images/fc_ends_with_bang.png' alt='dollar latex examples'>
      <figcaption>
        Examples of titles which end with <d-math>!</d-math> and the relevant conditional probabilities. Empirically,
        a
        <d-math>!</d-math> ending a title indicates a <d-math>89\%</d-math> chance that the paper is viXra, but only
        a tiny fraction of all papers end with an exclamation point.
      </figcaption>
    </figure>

    <p>
      Finally, logistic regression has picked up on the fact that viXra papers tend to be shorter than arXiv ones, as
      indicated by the fact that they typically have more left-padding by blank spaces when forced to be of length
      <d-code language='python'>seq_len = 128</d-code>. The figure below shows the weights assigned to a blank space
      for
      each of the <d-code language='python'>128</d-code> possible character positions in a title (top-figure) as well
      as
      the cumulative sum of these weights for all points up to and including each such position (bottom-figure). In
      the
      cumulative plot there is a small trend towards arXiv in the positions <d-math>\sim 20-60</d-math> range which
      then
      strongly reverses and starts to favor viXra around positions <d-math>\sim 75</d-math> and beyond. These
      inflection-points roughly correspond to the mean arXiv and viXra title lengths. As seen in the
      next section, character length is one of the primary predictors for a title's origin.
    </p>

    <figure style='grid-column-end: page-end' id='logistic-charlen'>
      <img src='images/balanced_title_one_hot_linear_blank_weights_by_position.png'
        alt='Weights for blank spaces, logistic regression.'>
      <figcaption>
        Plots of the individual and cumulative weights for blank space by position. The average arXiv title starts
        having non-trivial characters at the green line, while the average viXra title starts at the red.
      </figcaption>
    </figure>

    <h3>
      Random Forest
    </h3>

    <p>
      I additionally trained a Random Forest<d-footnote id='rf'>
        In brief, a Random Forest classifier works by generating a large number of decision trees which are each trained
        using a
        randomly selected set of training examples and a randomly selected subset of data features. The conditions for
        generating a terminating leaf
        are controlled by various hyperparameters, such as the maximum depth of the tree or number of training examples
        remaining post-split. In a binary classification context, the
        Random Forest then makes a prediction for a new data
        point by passing it through all of the collected trees and taking a majority vote based on the various leaves
        the data point settles in.
        The random selection of data points and features for each tree and the averaging procedure at inference time
        are the key features which decorrelate trees and combat variance.
      </d-footnote>, after performing some additional feature engineering, such as computing the frequency with which
      various forms of punctuation appear in titles and the length of the title's longest word resulting in 64 total
      features. See the figure below for the data distributions along some particular engineered feature directions.
      The ultimate model (which only uses ten of the 64 features, discussed below) also achieved <d-math>\approx 70\%
      </d-math> accuracy on the validation set

    </p>

    <figure style='grid-column-end: page-end' id='rf-corner'>
      <img src='images/rf_feature_engineering_corner_plot.png' alt='feature engineering for the random forest'>
      <figcaption>
        A corner plot of a few features used by the Random Forest. <d-code language='python'>mean_word_rank</d-code>
        is
        the average rank of all words in each title, as computed from the ranks of how common each word is in the
        training set.
      </figcaption>
    </figure>


    <p>
      Sixty-five features is too many to be useful for a simple quick-and-dirty baseline. Apart from some quantitative
      benchmarks, we also want a greater understanding of what features are <em>important</em> in distinguishing arXiv
      papers from viXra, and most of the 64 are not.
    </p>

    <p>
      There exist two common methods for quantifying the importance of different features in a Random Forest:
    </p>

    <ul>
      <li>
        <b>Gini-Impurity-Based Importance:</b> in <d-code language='python'>sklearn</d-code>, the split-points in trees
        are determined by maximizing Gini Impurity<d-footnote id='gini'>
          The Gini Impurity of a set of data points belonging to <d-math>C</d-math> classes, indexed by <d-math>i
            \in\{0,\ldots, C-1\}</d-math>, is
          given by the probability of misclassifying a randomly chosen data point when using a simple strategy in
          which
          you
          randomly guess which class the point belongs to in proportion to the relative class populations of the whole
          set. That is, if <d-math>p_i</d-math> is the probability of drawing from class <d-math>i</d-math>, then
          the
          impurity is given by
          <d-math block=''>
            I_G = \sum_i P({\rm misclassify}|c_i)P(c_i) =\sum_i (1-p_i)p_i\ .
          </d-math>
          If the
          data set only has elements from one class, then <d-math>I_G</d-math> is minimized at zero, while an
          equally-balanced dataset has maximal <d-math>I_G</d-math>. A Random Forest strives to make splits such that
          all examples which end up in a leaf of a tree belong to the same class, which is equivalent to minimizing
          <d-math>I_G</d-math>.
        </d-footnote> decrease by default. By creating a weighted average of the Gini Impurity decrease achieved by
        every split
        in every tree per split-upon-feature, one estimates the relative importance of all features. <a target='_blank'
          rel='noopener noreferrer'
          href='https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_'>
          <d-code language='python'>feature_importances_</d-code>
        </a> is the<d-code language='python'>sklearn</d-code> implementation of this method.
      </li>
      <li>
        <b>Permutation-Based Importance</b>: a more computationally-expensive metric for assessing feature importance is
        to calculate the drop in performance which comes from randomly shuffling the values for a single feature in some
        validation set, repeating this process for each feature. <a target='_blank' rel='noopener noreferrer'
          href='https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance'>
          <d-code language='python'>permutation_importance</d-code>
        </a>
        is the <d-code language='python'>sklearn</d-code> implementation of this method.
      </li>
    </ul>

    <p>
      The Gini-Impurity-based method <a target='_blank' rel='noopener noreferrer'
        href='https://explained.ai/rf-importance/index.html'>has been criticized</a> as overestimating the importance of
      features which allow for many possible choices of split-points. As done in the preceding link, this effect can be
      seen by adding a test-feature which consists solely of randomly generated numbers. Adding such a feature to the
      data and computing feature importances as above, the Gini method ranked this
      <d-code language='python'>random</d-code> feature as the 17th most-important feature, while permutation importance
      ranked it dead-last: 65 of 65. See the plot of permutation-based importances below.
    </p>

    <figure style='grid-column-end: page-end' id='perm-import-all'>
      <img src='images/rf_0_perm_import_plt.png' alt='plot of all permutation based importances'>
      <figcaption>
        The losses in accuracy when different feature dimensions are shuffled. Multiple shuffle-iterations are performed
        for each feature and the error bars come from the standard deviation across shuffles. Shuffling the
        <d-code language='python'>random</d-code> feature improves performance, presumably since it washes out the
        unwanted effects created by training on this uninformative column in the first place.
      </figcaption>
    </figure>

    <p>
      There are clearly many uninformative features. As a quick-and-dirty contraction of the model, I re-trained the
      Random Forest on the top-ten<d-footnote id='removing-ten'>
        Reducing the number of features slightly <em>increased</em> the performance of the model. In general, one should
        also carefully consider the correlations between different features before winnowing down the elements which are
        fed into a Random Forest, but taking the top-ten was good-enough for present purposes.
      </d-footnote> features per the above chart, resulting in the more-focused importances plot below.
      As found in the logistic regression model, many of the most-informative markers regarded the use of punctuation
      marks and the overall length of the title. Unlike logistic regression, the Random Forest also had access to how
      common the words in each title were ( <d-code language='python'>mean_word_rank</d-code>), which also carried
      significant meaning, as one might have suspected.
    </p>

    <figure style='grid-column-end: page-end' id='perm-import-all'>
      <img src='images/rf_1p_perm_import_plt.png' alt='plot of all permutation based importances'>
      <figcaption>
        Permutation-based importances for the top-ten most important columns.
      </figcaption>
    </figure>

    <p>
      Lastly, it can be informative to get an inside-view of what is going on in the various trees which comprise a
      Random Forest.
      The <a target='_blank' rel='noopener noreferrer' href='https://github.com/parrt/dtreeviz'>
        <d-code language='python'>dtreeviz</d-code> package
      </a> is a useful tool for this purpose. There exist methods for visualizing how groups of data or individual
      titles filter through the leaves of the tree. An example of the path that one of my own papers takes when
      traversing a single tree is below. A visualization of 500 validation-set samples being filtered into the leaves
      of the same tree is a bit more unwieldy, <a target='_blank' rel='noopener noreferrer'
        href='images/rf_1p_rand_tree_full_viz.svg'>but can be found here.</a>
    </p>

    <figure style='grid-column-end: page-end' id='dtreeviz-goon'>
      <img src='images/rf_1p_rand_tree_viz.svg' alt='dtreeviz visualization of a single paper prediction'>
      <figcaption>
        An (incorrect) prediction-path for one of my papers (<a target='_blank' rel='noopener noreferrer'
          href='https://arxiv.org/abs/1201.0015'><em>
            Gauged Galileons From Branes</em></a>) from one tree in the Random Forest, as visualized by <d-code
          language='python'>dtreeviz</d-code>. The yellow bars indicate the distribution of
        all twenty of my papers, while the orange arrow points to the statistics of <em>
          Gauged Galileons From Branes</em> and the black arrow marks the split-points.
      </figcaption>
    </figure>




    <h3>
      Performance on My Papers
    </h3>

    <p>
      I, of course, have to check the model performance <a target='_blank' rel='noopener noreferrer'
        href='https://inspirehep.net/literature?sort=mostrecent&size=25&page=1&q=a%20g%20goon&ui-citation-summary=true'>on
        my own papers</a> (which are all arXiv), which I will do for each model in this series of posts.
    </p>

    <p>
      The results aren't great for these baselines! Both models only classify half of my papers correctly. As seen in
      the figures below, there
      is a clear trend for longer titles to be predicted as arXiv and shorter ones as viXra, as expected from the
      above. This might be expected, given the lack of
      <d-code language='python'>LaTeX</d-code> and related technical markers in the titles of my
      papers.
    </p>

    <figure style='grid-column-end: page-end' id='logistic-preds'>
      <img src='images/balanced_title_one_hot_linear_goon_papers_preds.png' alt='logistic preds'>
      <figcaption>
        Probabilities that each of my titles are viXra, as predicted by the linear model.
      </figcaption>
    </figure>

    <figure style='grid-column-end: page-end' id='rf-preds'>
      <img src='images/goon_papers_df_tree_pred_plot.png' alt='random forest predictions'>
      <figcaption>
        Probabilities that each of my titles are viXra, as predicted by the Random Forest, as estimated by computing
        the
        mean prediction over all trees in the forest.
      </figcaption>
    </figure>


  </d-article>



  <d-appendix>


    <h3>Acknowledgments</h3>

    <p>
      Thank you to <a href="https://distill.pub" target="_blank" rel="noopener noreferrer">the <em>Distill</em> team</a>
      for making their
      <a href="https://github.com/distillpub" target="_blank" rel="noopener noreferrer">article template publicly
        available.</a> Discussions with Matt Malloy and Rami Vanguri for gaining an intuition for the various
      hyperparameters used in a Random Forest.
    </p>


    <h3>Helpful Links and Resources</h3>


    <ul class="color-dot-ul">
      <li>
        Jeremy Howard has great videos on the practicalities of (and some theory behind) Random Forests, such as <a
          target='_blank' rel='noopener noreferrer'
          href='https://www.youtube.com/watch?v=blyXCk4sgEg&list=PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96&index=2'>here</a>
        and <a target='_blank' rel='noopener noreferrer'
          href='https://www.youtube.com/watch?v=VEG5xT5gAHc&list=PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM&index=7'>here</a>.
        Notes on Random Forest hyperparameter tuning from his <d-code language='python'>fastai</d-code> course <a
          target='_blank' rel='noopener noreferrer' href='https://forums.fast.ai/t/wiki-lesson-thread-lesson-4/7540'>can
          be found here</a>.
      </li>
      <li>
        Random forests are also useful for analyzing the performance other ML architectures. Weights and Biases (used
        extensively for
        this project, see the <a target='_blank' rel='noopener noreferrer'
          href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow post</a>) uses Random Forests to estimate the
        importance of various
        hyperparameters across model training runs <a target='_blank' rel='noopener noreferrer'
          href='https://docs.wandb.ai/ref/app/features/panels/parameter-importance#interpreting-a-hyperparameter-importance-panel'>via
          the permutation-based method detailed above</a>.
      </li>
    </ul>

    <d-footnote-list></d-footnote-list>

    <h3>All Project Posts</h3>

    <p>Links to all posts in this series.
      <em>Note: all code for this project can be found <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml'>on my GitHub page</a>.
      </em>
    </p>

    <ul>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-data/">The Data</a>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-baseline-models/">Baseline Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-recurrent/">Simple
          Recurrent Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-embeddings/">Embeddings
          (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-convolutions/">Convolutions (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-attention/">Attention (To
          Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transformers/">Transformers (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transfer-learning/">Transfer Learning (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-test-set-conclusions/">Test Set Performance and Conclusions (To
          Come)</a>
      </li>
    </ul>
    <d-citation-list></d-citation-list>
  </d-appendix>


  <!-- bibliography will be inlined during Distill pipeline's pre-rendering
    (GG: I have not managed to get the bibliography to compile after the ejs
     is converted to a static html file, so commenting out)
  <d-bibliography src="bibliography.bib"></d-bibliography>

   -->


</body>